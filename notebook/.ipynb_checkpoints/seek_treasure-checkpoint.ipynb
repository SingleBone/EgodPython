{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o----T"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egod/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n",
      "will be corrected to return the positional maximum in the future.\n",
      "Use 'series.values.argmax' to get the position of the maximum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           \n",
      "Q-table:\n",
      "\n",
      "       left     right\n",
      "0  0.000011  0.006861\n",
      "1  0.000360  0.029792\n",
      "2  0.000152  0.119667\n",
      "3  0.000381  0.375384\n",
      "4  0.031646  0.745813\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DF\n",
    "import time\n",
    "\n",
    "# 超参数/全局参数\n",
    "\n",
    "N_STATES = 6 #世界长度/状态个数\n",
    "ACTIONS = ['left','right'] \n",
    "EPSILON = 0.9 # 贪婪度\n",
    "ALPHA = 0.1 # 学习率\n",
    "GAMMA = 0.9 # 奖励递减/衰退系数\n",
    "MAX_EPISODES = 13 # 最大回合数\n",
    "FRESH_TIME = 0.3 # 回合间隔时间\n",
    "\n",
    "# Q表生成函数\n",
    "\n",
    "def build_Q_table(n_states,actions):\n",
    "    table = DF(np.zeros((n_states,len(actions))),columns=actions) \n",
    "    return table\n",
    "\n",
    "# 行为选择函数\n",
    "\n",
    "def choose_action(state,q_table):\n",
    "    state_actions = q_table.iloc[state,:] #检索出状态为state时的动作倾向直\n",
    "    choice = np.random.uniform() #随机选择直\n",
    "    if (choice > EPSILON) or (state_actions.all() == 0): # 当随即选择直 > 贪婪度 或者 第一次进入该state时，随机选择行动\n",
    "        action = np.random.choice(ACTIONS)\n",
    "    else: # 否则按照动作倾向直选择行动\n",
    "        action = state_actions.argmax()\n",
    "    return action\n",
    "\n",
    "# 环境反馈函数\n",
    "\n",
    "def env_feedback(state,action):\n",
    "    if action == 'right': # 如果行动为向右\n",
    "        if state == N_STATES-2: # 如果紧贴终点，则胜利并获得奖励\n",
    "            state_pre = 'terminal'\n",
    "            reward = 1\n",
    "        else: # 否则向右走一步，无奖励\n",
    "            state_pre = state + 1\n",
    "            reward = 0\n",
    "    else: # 如果向左\n",
    "        reward=0 # 向左没有奖励\n",
    "        if state == 0: # 如果已经靠墙，则原地踏步\n",
    "            state_pre = state\n",
    "        else: # 否则向左走一步\n",
    "            state_pre = state - 1\n",
    "    return state_pre,reward\n",
    "\n",
    "# 环境更新函数\n",
    "\n",
    "def update_env(state,episode,step_counter):\n",
    "    env_list = ['-']*(N_STATES-1)+['T'] # 环境 = ------T\n",
    "    if state == 'terminal': # 如果已经达到终点， 宣告回合结束并展示统计信息\n",
    "        interaction = 'Episode : %s Total steps : %s'%(episode+1,step_counter)\n",
    "        print('\\r{}'.format(interaction),end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r           ',end='')\n",
    "    else: # 否则，在冒险者所在位置 用 o 代替 - 表示其位置\n",
    "        env_list[state] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction),end='')\n",
    "        time.sleep(FRESH_TIME)\n",
    "\n",
    "# 强化学习主循环\n",
    "\n",
    "def RL():\n",
    "    q_table = build_Q_table(N_STATES,ACTIONS) # 初始化Q表\n",
    "    \n",
    "    for episode in range(MAX_EPISODES): #循环 MAX_EPISODES 回合\n",
    "        step_counter = 0 # 初始步数\n",
    "        state = 0 # 冒险者初始位置为最左边\n",
    "        is_terminated = False # 初始胜利flag\n",
    "        update_env(state,episode,step_counter) # 初始环境\n",
    "        \n",
    "        while not is_terminated: # 未触发胜利flag时，游戏一直循环进行\n",
    "            action = choose_action(state,q_table) # 选择动作\n",
    "            state_fb,reward = env_feedback(state,action) # 获得环境反馈：该state下采取该action后的下一步state_fb 和 奖励 reward\n",
    "            q_env = q_table.loc[state,action] # 当前该state下采取action的q_env直\n",
    "           \n",
    "            if state_fb != 'terminal': # 如果 state_fb 不是终点 \n",
    "                q_target = reward + GAMMA*q_table.iloc[state_fb,:].max() # 想象获得的q直\n",
    "            else: # 否则\n",
    "                q_target = reward \n",
    "                is_terminated = True\n",
    "        \n",
    "            q_table.loc[state,action] += ALPHA*(q_target-q_env) # 更新 q_table\n",
    "            state = state_fb # 更新state\n",
    "        \n",
    "            update_env(state,episode,step_counter+1)\n",
    "            step_counter += 1\n",
    "    return q_table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q_table = RL()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
